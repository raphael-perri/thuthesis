% !TeX root = ../thuthesis-example.tex

\chapter{Sequence-based Method}

\section{Introduction}

As of today, only sequence-based model exists for the prediction of the Michaelis constant. Some (Km pred1) use
simple embeddings to represent the protein sequences. Other such as (ENKIE) use Bayesian Multilevel Models (BMMs)
to capture a simple hierarchy of enzyme properties. Finally, the state-of-the-art model ProSmith pre-trains a
large enzyme-specific protein models and uses embeddings from existing large protein models. 

In this chapter, we present SeqKm a deep learning model that makes use of existing large protein models without
pretraining. This not only allows for fast training and prediction as there is no need for pretraining but also
offer better performances by leveraging the protein knowledge retained by large protein models.

To demonstrate our method applicability to Michaelis constant prediction, we analyze the sequence-based
model performance compared to the state-of-the-art models in this field. Even though this model
does not outperform ProSmith, it shows equal performances on the new curated test set, indicating our
model's better ability to generalize and showing promissing results for the ensemble method.

\section{Methods}

We now describe our input representation, model achitecture, training details, and results. 
Figure \ref{fig:seqkm} provides an overview of our method.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{2-sequence_architecture.png}
  \caption{Overview of SeqKm}
  \label{fig:seqkm}
\end{figure}

\subsection{Input representation}

We are given a protein sequence of length $n$ $P=a_1a_2a_3...a_n$ where $a_i$ represents the amino acid
at position $i\in\{1,..,n\}$ and each $a_i$ is an element from the set of 20 standards amino acids to
which we added an unknown amino acid $X$: $A=\{A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y,X\}$.

As a string cannot directly be processed by a deep learning model, a tokenization step is necessary. As
the ProtT5-XL model will be used, its tokenizer will also be used to prepare the protein sequences \cite{prottrans}.
The ProtT5-XL tokenizer is made on a vocabulary containing all the 20 amino acids but also other unsure or less
common amino acids such as B that refers to asparagine (N) and aspartic acid (D) when the specific
amino acid cannot be determined. It also has a padding token <pad>, an unknown token <unk>, a end
token </s> to indicate the end of the sequence, and some additional tokens for special cases.

Hence, all protein sequences are first truncated or padded to length 1023 and then tokenized to have inputs of
length 1024: 1023 amino acids or pad tokens <pad> and the end token </s>. This input can be used for downstream
applications.

We are also given a substrate string in its SMILES form. This can be defined as a $S=c_1c_2c_3...c_n$
where $c_i$ represents the chemical symbol for the atom at position $i\in\{1,...,n\}$ in the SMILES
sequence, or a symbol representing a bond or branching in the molecule.

Similarly, it first needs to be processed in a format that can be processed by a statistical model. To do so,
we use the Morgan Fingerprints that represent a small molecule in a way that can easily be used for computational
tasks such as ours. We selected a vector size of 1024 to be consistent with the size of the protein sequence. We
also choose to use a radius size of 1, indicating that each atom of the molecule is updated only once based on its
immediate neighbors.

We know have two inputs that can be processed by a machine learning model.

\subsection{Model Architecture}

Our model is composed of three modules: a protein encoder that encode the protein, a substrate encoder that
encode the substrate, and an interactor that makes the encoded protein and the encoded substrate interact.

The protein encoder is composed of the ProtT5-XL model to which we relaxed the 2 last layers during the training.
Relaxing is a selective fine-tuning strategy that allows to slightly adjust certains weights of a pretrained model 
to better perfom on our specific task. The decision of relaxing only the 2 last layers of the model is based on
the assumption that these layers contain the most task-specific information, while ealiest layers usually capture
universal features. This not only render the training faster are only a few weights need to be updated during the
fine-tuning process while still making the model adapted to our specific task.

The protein encoder output a matrix of size $1024\times 1024$ with the first one being the length of the sequence,
padded or not, and the second being the embedding dimension which is 1024. Technically, this mean that every
amino acid has an embedding of size 1024. Considering we are only looking for an embedding per protein sequence
and not per amino acid, we use the common technique of averaging on the amino acid dimension, leaving us with a
single vector of length 1024.

The substrate encoder for this model is simply a dummy model that does not modify the input as it was already
processed as a Morgan Fingerprints, which is already a reprensetation of the substrate as a vector of length 1024.

Finally, the interactor is a multilayer perceptron of 2 dense layers of size 256 and 128. It takes as input the
concatenated protein embedding and substrate embedding and pass it through these 2 layers to finally output a single
value, the Michaelis constant $K_m$.

\section{Training details}

We trained and validated our model using the dataset described in Section \ref{sec:dataset} as well as the new
test dataset that we currated and that contains data from 2022 to 2024. The training set contains roughly 8k
protein-substrate pairs and their $K_m$ value. The validation and testing set contains about 1k and 2k pairs
respectively. 

We trained our model over 300 epochs using the Adam optimizer with $\beta_1=0.9$ and $\beta_2=0.999$, and an initial
learning rate of $5\times10^{-4}$. The learning rate follows a cosine schedule, including a warmup phase of 200
steps to gradually increaser the learning rate from zero to the initial set value, ehnancing the model's convergence
stability. Training and validation batch sizes are set to 256 and 32 per device, respectively, to efficiently
utilize computational resources while maintaining an appropriate balance between speed and memory usage. 

Additionally, we leverage a cosine learning rate scheduler to adjust the learning rate dynamically, 
encouraging better fine-tuning and generalization as training progresses. To monitor the model's performance 
and ensure robustness, we log metrics every 20 steps and save the model's state every 500 steps. 
Evaluation is conducted at each step, allowing for continuous monitoring of the model's effectiveness on 
validation data. 

Finally, after looking at the best metrics for the validation test, we reverted to the saved model of the
 17th epoch. 

 \section{Results}

 Using the model and the training strategy presented above, we obtain a MSE of 0.700 and a $r^2$ of 0.493 
 as well as the following hot/cold results for the test set:

 \begin{table}[ht]
  \centering
  \begin{tabular}{lcccccc}
  \hline
   & \multicolumn{3}{c}{\textbf{Hot substrate}} & \multicolumn{3}{c}{\textbf{Cold substrate}} \\
   & Samples & MSE & R\(^2\) & Samples & MSE & R\(^2\) \\ \hline
  \textbf{Hot protein}  & 1192 & 0.642 & 0.485 & 64 & 0.821 & 0.449 \\
  \textbf{Cold protein} & 985 & 0.708 & 0.536 & 98 & 1.250 & -0.004 \\ \hline
  \end{tabular}
  \caption{SeqKm results on the test set}
  \label{tab:summary_performance}
 \end{table}



For the new test set, we obtain:

\begin{table}[ht]
  \centering
  \begin{tabular}{lcccccc}
  \hline
   & \multicolumn{3}{c}{\textbf{Hot substrate}} & \multicolumn{3}{c}{\textbf{Cold substrate}} \\
   & Samples & MSE & R\(^2\) & Samples & MSE & R\(^2\) \\ \hline
  \textbf{Hot protein} & 40 & 0.787 & 0.472 & 30 & 0.851 & -0.255 \\
  \textbf{Cold protein} & 1117 & 0.884 & 0.478 & 102 & 2.066 & -0.006 \\ \hline
  \end{tabular}
  \caption{SeqKm results on the new test set}
  \label{tab:updated_summary_performance}
\end{table}

Our method offer better performances only for 1 out of the 4 models of our benchmark, which indicates 
possible improvements.